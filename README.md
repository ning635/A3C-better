# A3C æ·±åº¦å¼ºåŒ–å­¦ä¹ --ç®—æ³•å¤ç°ä¸ä¼˜åŒ–

---

## ğŸ“š ç›®å½•

1. [A3Cç®—æ³•åŸç†](#1-a3cç®—æ³•åŸç†)
2. [ç¯å¢ƒé…ç½®æŒ‡å—](#2-ç¯å¢ƒé…ç½®æŒ‡å—)
3. [ä»£ç ç»“æ„è¯¦è§£](#3-ä»£ç ç»“æ„è¯¦è§£)
4. [è®­ç»ƒå¤ç°æ­¥éª¤](#4-è®­ç»ƒå¤ç°æ­¥éª¤)
5. [å¯è§†åŒ–ä¸ç»“æœåˆ†æ](#5-å¯è§†åŒ–ä¸ç»“æœåˆ†æ)
6. [ä¼˜åŒ–æ–¹æ³•è¯¦è§£](#6-ä¼˜åŒ–æ–¹æ³•è¯¦è§£)
7. [æ¸¸æˆæ¼”ç¤º](#7-æ¸¸æˆæ¼”ç¤º)
8. [å®Œæ•´å®éªŒæŠ¥å‘Š](#8-å®Œæ•´å®éªŒæŠ¥å‘Š)

---

## 1. A3Cç®—æ³•åŸç†

### 1.1 ä»€ä¹ˆæ˜¯A3Cï¼Ÿ

**A3C (Asynchronous Advantage Actor-Critic)** æ˜¯DeepMindåœ¨2016å¹´æå‡ºçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

**A3C = å¼‚æ­¥(Asynchronous) + ä¼˜åŠ¿å‡½æ•°(Advantage) + æ¼”å‘˜-è¯„è®ºå®¶(Actor-Critic)**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Global Network     â”‚
                    â”‚   (å…¨å±€å…±äº«æ¨¡å‹)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                   â”‚                   â”‚
           â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Worker 1 â”‚        â”‚ Worker 2 â”‚        â”‚ Worker N â”‚
    â”‚ 1.å¤åˆ¶å‚æ•°â”‚        â”‚ 1.å¤åˆ¶å‚æ•°â”‚        â”‚ 1.å¤åˆ¶å‚æ•°â”‚
    â”‚ 2.ç©æ¸¸æˆ  â”‚        â”‚ 2.ç©æ¸¸æˆ  â”‚        â”‚ 2.ç©æ¸¸æˆ  â”‚
    â”‚ 3.è®¡ç®—æ¢¯åº¦â”‚        â”‚ 3.è®¡ç®—æ¢¯åº¦â”‚        â”‚ 3.è®¡ç®—æ¢¯åº¦â”‚
    â”‚ 4.æ›´æ–°å…¨å±€â”‚        â”‚ 4.æ›´æ–°å…¨å±€â”‚        â”‚ 4.æ›´æ–°å…¨å±€â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

#### ğŸ­ Actor-Criticæ¶æ„

| ç»„ä»¶             | ä½œç”¨                   | è¾“å‡º         |
| ---------------- | ---------------------- | ------------ |
| **Actor**  | å­¦ä¹ ç­–ç•¥ï¼Œå†³å®šåŠ¨ä½œæ¦‚ç‡ | $\pi(a|s)$ |
| **Critic** | è¯„ä¼°çŠ¶æ€ä»·å€¼           | $V(s)$     |

#### ğŸ“Š ä¼˜åŠ¿å‡½æ•°

$$
A(s, a) = Q(s, a) - V(s) = r + \gamma V(s') - V(s)
$$

ä¼˜åŠ¿å‡½æ•°è¡¡é‡"è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å¥½å¤šå°‘"ã€‚

#### ğŸ”„ å¼‚æ­¥è®­ç»ƒ

å¤šä¸ªworkerå¹¶è¡Œæ”¶é›†ç»éªŒï¼Œå¼‚æ­¥æ›´æ–°å…¨å±€æ¨¡å‹ï¼Œæ‰“ç ´æ ·æœ¬ç›¸å…³æ€§ã€‚

### 1.3 æŸå¤±å‡½æ•°

```
æ€»æŸå¤± = ç­–ç•¥æŸå¤± + 0.5 Ã— ä»·å€¼æŸå¤± - 0.01 Ã— ç†µ

å…¶ä¸­ï¼š
- ç­–ç•¥æŸå¤± = -log(Ï€(a|s)) Ã— A(s,a)  # ä½¿å¥½åŠ¨ä½œæ¦‚ç‡æ›´é«˜
- ä»·å€¼æŸå¤± = (R - V(s))Â²            # ä½¿ä»·å€¼é¢„æµ‹æ›´å‡†ç¡®
- ç†µæŸå¤± = -Î£ Ï€(a|s) Ã— log(Ï€(a|s))  # é¼“åŠ±æ¢ç´¢
```

---

## 2. ç¯å¢ƒé…ç½®æŒ‡å—

### 2.1 ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windows 10/11, Linux, macOS
- **Python**: 3.8 - 3.10
- **CPU**: 4æ ¸ä»¥ä¸Š

### 2.2 åˆ›å»ºç¯å¢ƒ

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n a3c python=3.9 -y
conda activate a3c
```

### 2.3 å®‰è£…ä¾èµ–

```bash
# PyTorch
pip install torch torchvision

# Gymnasium (Atariæ¸¸æˆ)
pip install gymnasium
pip install gymnasium[atari]
pip install gymnasium[accept-rom-license]
pip install ale-py

# è¾…åŠ©åº“
pip install opencv-python matplotlib numpy imageio
```

### 2.4 éªŒè¯å®‰è£…

```python
import gymnasium as gym
import torch

env = gym.make("PongNoFrameskip-v4")
print(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
print(f"åŠ¨ä½œç©ºé—´: {env.action_space}")
print(f"PyTorch: {torch.__version__}")
env.close()
```

---

## 3. ä»£ç ç»“æ„è¯¦è§£

### 3.1 é¡¹ç›®ç»“æ„

```
a3c-better/
â”œâ”€â”€ main.py              # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ main_with_log.py     # å¸¦æ—¥å¿—çš„ä¸»ç¨‹åº
â”œâ”€â”€ model.py             # ç¥ç»ç½‘ç»œæ¨¡å‹
â”œâ”€â”€ train.py             # è®­ç»ƒå·¥ä½œè¿›ç¨‹
â”œâ”€â”€ test.py              # æµ‹è¯•å·¥ä½œè¿›ç¨‹
â”œâ”€â”€ envs.py              # ç¯å¢ƒé¢„å¤„ç†
â”œâ”€â”€ visualize.py         # å¯è§†åŒ–è„šæœ¬
â”œâ”€â”€ show_game.py         # æ¸¸æˆæ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ analyze_results.py   # ç»“æœåˆ†æè„šæœ¬
â””â”€â”€ logs/                # è®­ç»ƒæ—¥å¿—
```

### 3.2 ç¥ç»ç½‘ç»œæ¨¡å‹ (model.py)

```python
"""
A3Cç¥ç»ç½‘ç»œæ¶æ„
è¾“å…¥: 4å¸§ç°åº¦å›¾åƒ (4 Ã— 42 Ã— 42)
è¾“å‡º: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ + çŠ¶æ€ä»·å€¼
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class ActorCritic(nn.Module):
    def __init__(self, num_inputs, action_space):
        super(ActorCritic, self).__init__()
  
        # å·ç§¯å±‚ï¼šæå–å›¾åƒç‰¹å¾
        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
  
        # LSTMï¼šè®°å¿†æ—¶åºä¿¡æ¯
        self.lstm = nn.LSTMCell(32 * 3 * 3, 256)
  
        # è¾“å‡ºå±‚
        self.critic_linear = nn.Linear(256, 1)           # ä»·å€¼å‡½æ•°
        self.actor_linear = nn.Linear(256, action_space.n)  # ç­–ç•¥
  
    def forward(self, inputs):
        inputs, (hx, cx) = inputs
  
        # å·ç§¯ç‰¹å¾æå–
        x = F.elu(self.conv1(inputs))
        x = F.elu(self.conv2(x))
        x = F.elu(self.conv3(x))
        x = F.elu(self.conv4(x))
  
        # å±•å¹³å¹¶é€šè¿‡LSTM
        x = x.view(-1, 32 * 3 * 3)
        hx, cx = self.lstm(x, (hx, cx))
  
        # è¾“å‡ºä»·å€¼å’Œç­–ç•¥
        value = self.critic_linear(hx)
        policy = self.actor_linear(hx)
  
        return value, policy, (hx, cx)
```

**ç½‘ç»œç»“æ„å›¾**:

```
è¾“å…¥: 4Ã—42Ã—42 (4å¸§å †å )
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv1: 32@3Ã—3   â”‚ â†’ 32Ã—21Ã—21
â”‚ stride=2        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ ELU
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv2: 32@3Ã—3   â”‚ â†’ 32Ã—11Ã—11
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ ELU
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv3: 32@3Ã—3   â”‚ â†’ 32Ã—6Ã—6
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ ELU
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv4: 32@3Ã—3   â”‚ â†’ 32Ã—3Ã—3 = 288
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Flatten
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LSTM: 256       â”‚ â†’ è®°å¿†æ—¶åºä¿¡æ¯
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Actor â”‚ â”‚Critic â”‚
â”‚ (6ä¸ª) â”‚ â”‚ (1ä¸ª) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 ç¯å¢ƒé¢„å¤„ç† (envs.py)

```python
"""
Atariç¯å¢ƒé¢„å¤„ç†
1. å›¾åƒç¼©æ”¾åˆ°42Ã—42
2. è½¬ä¸ºç°åº¦å›¾
3. å †å 4å¸§ï¼ˆæä¾›æ—¶åºä¿¡æ¯ï¼‰
4. å½’ä¸€åŒ–åˆ°[0,1]
"""
import cv2
import gymnasium as gym
import numpy as np
from collections import deque

def create_atari_env(env_id):
    """åˆ›å»ºé¢„å¤„ç†åçš„Atariç¯å¢ƒ"""
    env = gym.make(env_id)
    env = AtariRescale42x42(env)  # ç¼©æ”¾
    env = FrameStack(env, 4)      # å¸§å †å 
    env = NormalizedEnv(env)      # å½’ä¸€åŒ–
    return env

class FrameStack(gym.Wrapper):
    """å †å æœ€è¿‘kå¸§ä½œä¸ºè§‚å¯Ÿ"""
    def __init__(self, env, k=4):
        super().__init__(env)
        self.k = k
        self.frames = deque([], maxlen=k)
  
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        for _ in range(self.k):
            self.frames.append(obs)
        return self._get_obs(), info
  
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        self.frames.append(obs)
        return self._get_obs(), reward, terminated, truncated, info
  
    def _get_obs(self):
        # å †å 4å¸§: (1,42,42) Ã— 4 â†’ (4,42,42)
        return np.concatenate(list(self.frames), axis=0)
```

**å¸§å †å çš„é‡è¦æ€§**:

- å•å¸§å›¾åƒæ— æ³•è¡¨ç¤ºè¿åŠ¨æ–¹å‘
- 4å¸§å †å è®©ç½‘ç»œèƒ½"çœ‹åˆ°"çƒçš„è¿åŠ¨è½¨è¿¹
- è¿™æ˜¯A3Cèƒ½å¤Ÿå­¦ä¹ çš„**å…³é”®**ï¼

### 3.4 è®­ç»ƒè¿›ç¨‹ (train.py)

```python
"""
A3Cè®­ç»ƒå·¥ä½œè¿›ç¨‹
æ¯ä¸ªworkerç‹¬ç«‹ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒå¹¶æ›´æ–°å…¨å±€æ¨¡å‹
"""
import torch
import torch.nn.functional as F

def train(rank, args, shared_model, counter, lock, optimizer):
    """è®­ç»ƒå·¥ä½œè¿›ç¨‹"""
  
    # åˆ›å»ºæœ¬åœ°æ¨¡å‹å’Œç¯å¢ƒ
    env = create_atari_env(args.env_name)
    model = ActorCritic(env.observation_space.shape[0], env.action_space)
  
    state = env.reset()
    state = torch.from_numpy(state)
    done = True
  
    while True:
        # 1. åŒæ­¥æœ¬åœ°æ¨¡å‹ä¸å…¨å±€æ¨¡å‹
        model.load_state_dict(shared_model.state_dict())
  
        # åˆå§‹åŒ–LSTMéšè—çŠ¶æ€
        if done:
            cx = torch.zeros(1, 256)
            hx = torch.zeros(1, 256)
  
        # 2. æ”¶é›†ç»éªŒ (Rollout)
        values, log_probs, rewards, entropies = [], [], [], []
  
        for step in range(args.num_steps):  # é»˜è®¤20æ­¥
            # å‰å‘ä¼ æ’­
            value, logit, (hx, cx) = model((state.unsqueeze(0), (hx, cx)))
      
            # é‡‡æ ·åŠ¨ä½œ
            prob = F.softmax(logit, dim=-1)
            log_prob = F.log_softmax(logit, dim=-1)
            action = prob.multinomial(num_samples=1)
      
            # è®¡ç®—ç†µï¼ˆç”¨äºæ¢ç´¢ï¼‰
            entropy = -(log_prob * prob).sum(1)
      
            # ä¸ç¯å¢ƒäº¤äº’
            state, reward, done, _ = env.step(action.item())
            reward = max(min(reward, 1), -1)  # å¥–åŠ±è£å‰ª
      
            # ä¿å­˜æ•°æ®
            values.append(value)
            log_probs.append(log_prob.gather(1, action))
            rewards.append(reward)
            entropies.append(entropy)
      
            if done:
                state = env.reset()
                break
      
            state = torch.from_numpy(state)
  
        # 3. è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
        R = 0 if done else model((state.unsqueeze(0), (hx, cx)))[0]
  
        policy_loss = 0
        value_loss = 0
        gae = 0  # å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡
  
        for i in reversed(range(len(rewards))):
            R = args.gamma * R + rewards[i]
            advantage = R - values[i]
            value_loss += 0.5 * advantage.pow(2)
      
            # GAE
            delta = rewards[i] + args.gamma * values[i+1] - values[i]
            gae = gae * args.gamma * args.gae_lambda + delta
      
            policy_loss -= log_probs[i] * gae - args.entropy_coef * entropies[i]
  
        # 4. åå‘ä¼ æ’­å¹¶æ›´æ–°å…¨å±€æ¨¡å‹
        optimizer.zero_grad()
        total_loss = policy_loss + args.value_loss_coef * value_loss
        total_loss.backward()
  
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)
  
        # å°†æ¢¯åº¦åº”ç”¨åˆ°å…±äº«æ¨¡å‹
        for param, shared_param in zip(model.parameters(), shared_model.parameters()):
            shared_param._grad = param.grad
  
        optimizer.step()
```

---

## 4. è®­ç»ƒå¤ç°æ­¥éª¤

### 4.1 åŸºçº¿è®­ç»ƒ

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate a3c

# åŸºæœ¬è®­ç»ƒï¼ˆ4ä¸ªworkerï¼‰
python main.py --env-name PongNoFrameskip-v4 --num-processes 4

# å¸¦æ—¥å¿—è®°å½•çš„è®­ç»ƒï¼ˆæ¨èï¼‰
python main_with_log.py --env-name PongNoFrameskip-v4 --num-processes 4 --log-dir logs
```

### 4.2 è¶…å‚æ•°è¯´æ˜

| å‚æ•°                  | é»˜è®¤å€¼ | è¯´æ˜           |
| --------------------- | ------ | -------------- |
| `--lr`              | 0.0001 | å­¦ä¹ ç‡         |
| `--gamma`           | 0.99   | æŠ˜æ‰£å› å­       |
| `--gae-lambda`      | 1.00   | GAEå‚æ•°        |
| `--entropy-coef`    | 0.01   | ç†µæ­£åˆ™åŒ–ç³»æ•°   |
| `--value-loss-coef` | 0.5    | ä»·å€¼æŸå¤±ç³»æ•°   |
| `--max-grad-norm`   | 50     | æ¢¯åº¦è£å‰ªé˜ˆå€¼   |
| `--num-processes`   | 4      | è®­ç»ƒè¿›ç¨‹æ•°     |
| `--num-steps`       | 20     | æ¯æ¬¡æ›´æ–°çš„æ­¥æ•° |

### 4.3 è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ‰“å°ï¼š

```
Time 0h 10m 30s, num steps 50000, FPS 80, episode reward 15.0, avg(10) 12.5, best_avg 12.5
```

### 4.4 è®­ç»ƒç›®æ ‡

| é˜¶æ®µ     | å¥–åŠ±é˜ˆå€¼ | é¢„æœŸæ—¶é—´  |
| -------- | -------- | --------- |
| åˆæ­¥å­¦ä¹  | 0        | 10-15åˆ†é’Ÿ |
| åŸºæœ¬æŒæ¡ | 10       | 20-30åˆ†é’Ÿ |
| è®ºæ–‡æ ‡å‡† | 18       | 1-2å°æ—¶   |
| æ¥è¿‘å®Œç¾ | 20+      | 2-3å°æ—¶   |

---

## 5. å¯è§†åŒ–ä¸ç»“æœåˆ†æ

### 5.1 è®­ç»ƒæ—¥å¿—è®°å½•

**ä»£ç **: `test_with_log.py`

```python
import json
from datetime import datetime

# æ—¥å¿—æ•°æ®ç»“æ„
log_data = {
    "env_name": args.env_name,
    "start_time": datetime.now().strftime("%Y%m%d_%H%M%S"),
    "hyperparameters": {
        "lr": args.lr,
        "gamma": args.gamma,
        "entropy_coef": args.entropy_coef,
    },
    "episodes": []
}

# æ¯ä¸ªEpisodeè®°å½•
episode_data = {
    "episode": episode_count,
    "elapsed_time": elapsed_time,
    "total_steps": counter.value,
    "fps": counter.value / elapsed_time,
    "episode_reward": reward_sum,
    "avg_reward_10": avg_reward,
    "best_avg": best_avg,
    "episode_length": episode_length
}
log_data["episodes"].append(episode_data)

# ä¿å­˜åˆ°JSON
with open(log_file, 'w') as f:
    json.dump(log_data, f, indent=2)
```

### 5.2 è®­ç»ƒæ›²çº¿å¯è§†åŒ–

**ä»£ç **: `visualize.py`

```python
import matplotlib.pyplot as plt
import json
import numpy as np

def plot_training_curve(log_file, output_dir="figures"):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    with open(log_file, 'r') as f:
        log_data = json.load(f)
  
    episodes = log_data["episodes"]
    ep_nums = [ep["episode"] for ep in episodes]
    rewards = [ep["episode_reward"] for ep in episodes]
    avg_rewards = [ep["avg_reward_10"] for ep in episodes]
  
    plt.figure(figsize=(12, 8))
  
    # å¥–åŠ±æ›²çº¿
    plt.subplot(2, 2, 1)
    plt.plot(ep_nums, rewards, 'b-', alpha=0.3, label='Episode Reward')
    plt.plot(ep_nums, avg_rewards, 'r-', linewidth=2, label='10-Episode Avg')
    plt.axhline(y=18, color='green', linestyle='--', label='Target (18)')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Training Reward Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
  
    plt.savefig(f'{output_dir}/training_curves.png', dpi=150)
    print(f"Saved: {output_dir}/training_curves.png")
```

**ä½¿ç”¨æ–¹æ³•**:

```bash
python visualize.py logs/training_log_xxx.json -o figures
```

### 5.3 å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾

```python
def plot_reward_histogram(log_data, output_dir):
    """ç»˜åˆ¶å¥–åŠ±åˆ†å¸ƒ"""
    rewards = [ep["episode_reward"] for ep in log_data["episodes"]]
  
    plt.figure(figsize=(10, 5))
  
    # æ•´ä½“åˆ†å¸ƒ
    plt.subplot(1, 2, 1)
    plt.hist(rewards, bins=25, color='steelblue', edgecolor='white')
    plt.axvline(np.mean(rewards), color='red', linestyle='--', 
                label=f'Mean: {np.mean(rewards):.1f}')
    plt.xlabel('Episode Reward')
    plt.ylabel('Frequency')
    plt.title('Reward Distribution')
    plt.legend()
  
    # è®­ç»ƒå‰åå¯¹æ¯”
    plt.subplot(1, 2, 2)
    n = len(rewards)
    early = rewards[:n//4]
    late = rewards[-n//4:]
    plt.hist(early, bins=15, alpha=0.5, label='Early', color='red')
    plt.hist(late, bins=15, alpha=0.5, label='Late', color='green')
    plt.title('Early vs Late Training')
    plt.legend()
  
    plt.savefig(f'{output_dir}/reward_histogram.png', dpi=150)
```

### 5.4 å®Œæ•´åˆ†æ

```bash
# è¿è¡Œå®Œæ•´åˆ†æè„šæœ¬
python analyze_results.py logs/sample_baseline.json logs/sample_optimized.json
```

**è¾“å‡ºç¤ºä¾‹**:

![è®­ç»ƒè¯¦ç»†åˆ†æ](figures/baseline_detailed_analysis.png)

---

## 6. ä¼˜åŒ–æ–¹æ³•è¯¦è§£

### 6.1 åŸå§‹A3Cçš„ä¸è¶³

1. **é«˜æ–¹å·®**: ä¼˜åŠ¿ä¼°è®¡æ–¹å·®å¤§ï¼Œè®­ç»ƒä¸ç¨³å®š
2. **æ ·æœ¬æ•ˆç‡ä½**: æ¯ä¸ªæ ·æœ¬åªç”¨ä¸€æ¬¡
3. **è¶…å‚æ•°æ•æ„Ÿ**: å¯¹å­¦ä¹ ç‡ç­‰å‚æ•°æ•æ„Ÿ

### 6.2 ä¼˜åŒ–æ–¹æ³•ï¼šä¼˜åŠ¿å½’ä¸€åŒ–

**åŸç†**: å°†ä¼˜åŠ¿å‡½æ•°æ ‡å‡†åŒ–åˆ°å‡å€¼=0ï¼Œæ ‡å‡†å·®=1

```python
def train_optimized(rank, args, shared_model, counter, lock, optimizer):
    """ä¼˜åŒ–ç‰ˆè®­ç»ƒ - æ·»åŠ ä¼˜åŠ¿å½’ä¸€åŒ–"""
  
    # ... æ”¶é›†ç»éªŒä»£ç ç›¸åŒ ...
  
    # è®¡ç®—æ‰€æœ‰ä¼˜åŠ¿
    advantages = []
    gae = torch.zeros(1, 1)
  
    for i in reversed(range(len(rewards))):
        delta = rewards[i] + args.gamma * values[i+1] - values[i]
        gae = gae * args.gamma * args.gae_lambda + delta
        advantages.insert(0, gae)
  
    advantages = torch.cat(advantages)
  
    # â˜… ä¼˜åŠ¿å½’ä¸€åŒ–ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
    if len(advantages) > 1:
        adv_mean = advantages.mean()
        adv_std = advantages.std()
        advantages = (advantages - adv_mean) / (adv_std + 1e-8)
  
    # ä½¿ç”¨å½’ä¸€åŒ–åçš„ä¼˜åŠ¿è®¡ç®—ç­–ç•¥æŸå¤±
    for i in range(len(rewards)):
        policy_loss -= log_probs[i] * advantages[i].detach()
```

### 6.3 å¯¹æ¯”å¯è§†åŒ–

**ä»£ç **: `compare_versions.py`

```python
def plot_comparison(baseline_log, optimized_log, output_dir):
    """ç»˜åˆ¶å¯¹æ¯”å›¾"""
    with open(baseline_log) as f:
        baseline = json.load(f)
    with open(optimized_log) as f:
        optimized = json.load(f)
  
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
  
    # å­¦ä¹ æ›²çº¿å¯¹æ¯”
    ax1 = axes[0]
    ax1.plot([e['episode'] for e in baseline['episodes']], 
             [e['avg_reward_10'] for e in baseline['episodes']], 
             'b-', label='Baseline')
    ax1.plot([e['episode'] for e in optimized['episodes']], 
             [e['avg_reward_10'] for e in optimized['episodes']], 
             'r-', label='Optimized')
    ax1.axhline(y=18, color='green', linestyle='--')
    ax1.legend()
    ax1.set_title('Learning Curve Comparison')
  
    # ... æ›´å¤šå¯¹æ¯”å›¾ ...
  
    plt.savefig(f'{output_dir}/comparison.png', dpi=150)
```

**ä½¿ç”¨æ–¹æ³•**:

```bash
python compare_versions.py \
    --baseline-log logs/sample_baseline.json \
    --optimized-log logs/sample_optimized.json
```

### 6.4 ä¼˜åŒ–æ•ˆæœ

![ä¼˜åŒ–æ•ˆæœå¯¹æ¯”](figures/optimization_effect.png)

| æŒ‡æ ‡     | Baseline | Optimized | æå‡             |
| -------- | -------- | --------- | ---------------- |
| æœ€ç»ˆå¹³å‡ | 15.9     | 20.3      | **+27.7%** |
| è¾¾åˆ°0åˆ†  | 20 ep    | 14 ep     | 30%æ›´å¿«          |
| è¾¾åˆ°18åˆ† | 30 ep    | 23 ep     | 23%æ›´å¿«          |

---

## 7. æ¸¸æˆæ¼”ç¤º

### 7.1 å®æ—¶æ¸²æŸ“æ¸¸æˆç”»é¢

**ä»£ç **: `show_game.py`

```python
"""
å®æ—¶æ¸²æŸ“Pongæ¸¸æˆç”»é¢
"""
import gymnasium as gym
import torch
import numpy as np

def play_with_render(args):
    """å®æ—¶æ¸²æŸ“æ¸¸æˆ"""
    # åˆ›å»ºå¯æ¸²æŸ“çš„ç¯å¢ƒ
    env = gym.make(args.env_name, render_mode="human")
  
    # åŠ è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    model = load_model(args.model) if args.model else None
  
    for episode in range(args.episodes):
        obs, info = env.reset()
        done = False
        episode_reward = 0
  
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            if model:
                action = model.select_action(obs)
            else:
                action = env.action_space.sample()
      
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            episode_reward += reward
  
        print(f"Episode {episode+1}: Reward = {episode_reward}")
  
    env.close()
```

### 7.2 ä½¿ç”¨æ–¹æ³•

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate a3c

# å®æ—¶æ¸²æŸ“ï¼ˆä¼šå¼¹å‡ºæ¸¸æˆçª—å£ï¼‰
python show_game.py --episodes 3

# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
python show_game.py --model saved_model/model.pt --episodes 5

# å½•åˆ¶GIF
python show_game.py --record --episodes 3
```

### 7.3 å½•åˆ¶GIF

```python
def record_gif(args):
    """å½•åˆ¶æ¸¸æˆGIF"""
    import imageio
  
    env = gym.make(args.env_name, render_mode="rgb_array")
    frames = []
  
    obs, info = env.reset()
    done = False
  
    while not done:
        action = env.action_space.sample()  # æˆ–ä½¿ç”¨æ¨¡å‹
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
  
        # ä¿å­˜å¸§
        frame = env.render()
        frames.append(frame)
  
    # ä¿å­˜GIF
    imageio.mimsave('gameplay.gif', frames, fps=30)
    print("Saved: gameplay.gif")
```

**å½•åˆ¶å‘½ä»¤**:

```bash
python show_game.py --record --episodes 1
# è¾“å‡º: videos/pong_ep1_reward-21.gif
```

### 7.4 æ¸¸æˆç”»é¢å±•ç¤º

ä¸‹é¢æ˜¯AIç©Pongæ¸¸æˆçš„å®é™…å½•åƒï¼š

![Pongæ¸¸æˆæ¼”ç¤º](videos/pong_ep1_reward-21.gif)

> ğŸ“º ä¸Šå›¾å±•ç¤ºäº†A3Cæ™ºèƒ½ä½“ä¸æ¸¸æˆAIå¯¹æˆ˜çš„å®é™…ç”»é¢ã€‚ç»¿è‰²æ˜¯AIæ§åˆ¶çš„çƒæ‹ï¼Œæ©™è‰²æ˜¯æ™ºèƒ½ä½“æ§åˆ¶çš„çƒæ‹ã€‚

---

## 8. å®Œæ•´å®éªŒæŠ¥å‘Š

### 8.1 è¿è¡Œåˆ†æ

```bash
python analyze_results.py logs/sample_baseline.json logs/sample_optimized.json
```

### 8.2 åˆ†ææŠ¥å‘Šè¾“å‡º

```
======================================================================
 A3C è®­ç»ƒç»“æœè¯¦ç»†åˆ†ææŠ¥å‘Š
======================================================================

ã€1. åŸºçº¿ç‰ˆæœ¬ (Baseline A3C)ã€‘
--------------------------------------------------
  æ€»Episodes: 30
  è®­ç»ƒæ—¶é—´: 30.0 åˆ†é’Ÿ
  æ€»æ­¥æ•°: 30,000
  å¥–åŠ±ç»Ÿè®¡:
    - æœ€é«˜: 20
    - æœ€ä½: -21
    - å¹³å‡: -1.87
    - æœ€ç»ˆ10epå¹³å‡: 15.90

ã€2. ä¼˜åŒ–ç‰ˆæœ¬ (Optimized A3C)ã€‘
--------------------------------------------------
  æ€»Episodes: 30
  è®­ç»ƒæ—¶é—´: 30.0 åˆ†é’Ÿ
  æ€»æ­¥æ•°: 30,000
  å¥–åŠ±ç»Ÿè®¡:
    - æœ€é«˜: 21
    - æœ€ä½: -20
    - å¹³å‡: 6.23
    - æœ€ç»ˆ10epå¹³å‡: 20.30

ã€3. å¯¹æ¯”åˆ†æã€‘
--------------------------------------------------
  æœ€ç»ˆå¹³å‡å¥–åŠ±æå‡: +4.40 (+27.7%)
  
  æ”¶æ•›é€Ÿåº¦å¯¹æ¯”:
    - è¾¾åˆ° 0åˆ†: Baseline=20ep, Optimized=14ep (+30% faster)
    - è¾¾åˆ° 10åˆ†: Baseline=24ep, Optimized=18ep (+25% faster)
    - è¾¾åˆ° 18åˆ†: Baseline=30ep, Optimized=23ep (+23% faster)

ã€4. ä¼˜åŒ–æ–¹æ³•æ€»ç»“ã€‘
--------------------------------------------------
  æŠ€æœ¯: ä¼˜åŠ¿å½’ä¸€åŒ– (Advantage Normalization)
  å…¬å¼: A_norm = (A - mean(A)) / (std(A) + Îµ)
  æ•ˆæœ:
    âœ“ æœ€ç»ˆæ€§èƒ½æå‡: +27.7%
    âœ“ æ”¶æ•›é€Ÿåº¦æ›´å¿«: 23-30%
    âœ“ è®­ç»ƒæ›´ç¨³å®š
======================================================================
```

### 8.3 ç”Ÿæˆçš„å›¾è¡¨

| å›¾è¡¨                                 | è¯´æ˜             |
| ------------------------------------ | ---------------- |
| `baseline_detailed_analysis.png`   | åŸºçº¿ç‰ˆæœ¬è¯¦ç»†åˆ†æ |
| `optimized_detailed_analysis.png`  | ä¼˜åŒ–ç‰ˆæœ¬è¯¦ç»†åˆ†æ |
| `comparison_detailed_analysis.png` | å¯¹æ¯”åˆ†æ         |
| `optimization_effect.png`          | ä¼˜åŒ–æ•ˆæœå›¾       |

---

## ğŸ“ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# ç¯å¢ƒæ¿€æ´»
conda activate a3c

# è®­ç»ƒ
python main_with_log.py --env-name PongNoFrameskip-v4 --num-processes 4

# å¯è§†åŒ–
python visualize.py logs/training_log_*.json -o figures

# å¯¹æ¯”åˆ†æ
python compare_versions.py --baseline-log logs/baseline.json --optimized-log logs/optimized.json

# å®Œæ•´åˆ†æ
python analyze_results.py logs/sample_baseline.json logs/sample_optimized.json

# æ¸¸æˆæ¼”ç¤ºï¼ˆå®æ—¶æ¸²æŸ“ï¼‰
python show_game.py --episodes 3

# å½•åˆ¶GIF
python show_game.py --record --episodes 1
```

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. Mnih, V., et al. "Asynchronous Methods for Deep Reinforcement Learning." ICML 2016.
2. Schulman, J., et al. "High-Dimensional Continuous Control Using Generalized Advantage Estimation." ICLR 2016.
3. [PyTorch A3C Implementation](https://github.com/ikostrikov/pytorch-a3c)

---

