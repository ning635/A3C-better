"""
A3Cè®­ç»ƒä»£ç  - ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡å®ç°
å‚è€ƒ: https://github.com/ikostrikov/pytorch-a3c
"""

import torch
import torch.nn.functional as F
from model import ActorCritic
from utils import frame_preprocessing, stack_frames, initialize_queue, skip_frames, plot_avg_scores
try:
    import gymnasium as gym
except ImportError:
    import gym
import numpy as np
from collections import deque


def ensure_shared_grads(model, shared_model):
    """å°†æœ¬åœ°æ¨¡å‹çš„æ¢¯åº¦å¤åˆ¶åˆ°å…±äº«æ¨¡å‹ - è¿™æ˜¯A3Cçš„å…³é”®
    
    æ³¨æ„ï¼šä½¿ç”¨_gradè€Œä¸æ˜¯gradï¼Œå› ä¸ºgradæ˜¯åªè¯»å±æ€§
    """
    for param, shared_param in zip(model.parameters(), shared_model.parameters()):
        if shared_param.grad is not None:
            return  # å¦‚æœå·²ç»æœ‰æ¢¯åº¦ï¼Œè¯´æ˜å¦ä¸€ä¸ªworkerå·²ç»æ›´æ–°è¿‡äº†
        shared_param._grad = param.grad


def train(rank, shared_model, params, optimizer, lock, counter, layers_, avg_ep, scores, scores_avg, flag_exit):
    """
    A3C workerè®­ç»ƒå‡½æ•°
    """
    torch.manual_seed(params['seed'] + rank)
    np.random.seed(params['seed'] + rank)
    
    # åˆ›å»ºç¯å¢ƒ
    # Prefer Gymnasium with canonical wrappers
    try:
        from atari_env import make_env
        env = make_env(params['env_name'], seed=(params['seed'] + rank), frame_stack=layers_['n_frames'])
    except Exception:
        env = gym.make(params['env_name'])
    actions_name = getattr(env.unwrapped, 'get_action_meanings', lambda: [])()
    
    print(f' ----- TRAIN PHASE (Worker {rank}) -----')
    
    # åˆ›å»ºæœ¬åœ°æ¨¡å‹
    model = ActorCritic(
        input_shape=layers_['n_frames'],
        layer1=layers_['hidden_dim1'],
        kernel_size1=layers_['kernel_size1'],
        stride1=layers_['stride1'],
        layer2=layers_['hidden_dim2'],
        kernel_size2=layers_['kernel_size2'],
        stride2=layers_['stride2'],
        fc1_dim=layers_['fc1'],
        lstm_dim=layers_['lstm_dim'],
        out_actor_dim=layers_['out_actor_dim'],
        out_critic_dim=layers_['out_critic_dim']
    )
    model.train()
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    # Initial state
    reset_out = env.reset(seed=(params['seed'] + rank))
    if isinstance(reset_out, tuple):
        obs, _ = reset_out
    else:
        obs = reset_out
    # Ensure HWC uint8
    import numpy as np
    if obs.dtype != np.uint8:
        obs = (obs * 255).astype(np.uint8) if obs.max() <= 1.0 else obs.astype(np.uint8)
    state = torch.from_numpy(obs)
    
    done = True
    episode_length = 0
    episode_reward = 0
    
    while True:
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥é€€å‡º
        if flag_exit.value == 1:
            print(f"Worker {rank} terminating...")
            break
        
        # åŒæ­¥æœ¬åœ°æ¨¡å‹å’Œå…±äº«æ¨¡å‹
        model.load_state_dict(shared_model.state_dict())
        
        # é‡ç½®LSTMçŠ¶æ€
        if done:
            hx = torch.zeros(1, layers_['lstm_dim'])
            cx = torch.zeros(1, layers_['lstm_dim'])
        else:
            hx = hx.detach()
            cx = cx.detach()
        
        # æ”¶é›†ç»éªŒçš„åˆ—è¡¨
        values = []
        log_probs = []
        rewards = []
        entropies = []
        
        # æ”¶é›† n æ­¥ç»éªŒ
        for step in range(params['rollout_size']):
            episode_length += 1
            
            # å‰å‘ä¼ æ’­ (éœ€è¦æ¢¯åº¦!)
            state_tensor = state.unsqueeze(0).permute(0, 3, 1, 2)
            logits, value, (hx, cx) = model((state_tensor, (hx, cx)))
            
            # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
            prob = F.softmax(logits, dim=-1)
            log_prob = F.log_softmax(logits, dim=-1)
            
            # è®¡ç®—ç†µ (æ­£å€¼)
            entropy = -(log_prob * prob).sum(1, keepdim=True)
            entropies.append(entropy)
            
            # é‡‡æ ·åŠ¨ä½œ
            action = prob.multinomial(num_samples=1).detach()
            log_prob_action = log_prob.gather(1, action)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            step_out = env.step(action.item())
            if len(step_out) == 5:
                next_obs, reward, terminated, truncated, info = step_out
                done = terminated or truncated
            else:
                next_obs, reward, done, info = step_out
            
            # é™åˆ¶æœ€å¤§episodeé•¿åº¦
            done = done or episode_length >= params['max_ep_length']
            
            # Clip rewardåˆ°[-1, 1]
            reward_clipped = max(min(reward, 1), -1)
            
            # è®°å½•
            episode_reward += reward
            
            # æ›´æ–°counter
            with counter.get_lock():
                counter.value += 1
            
            # å¦‚æœepisodeç»“æŸ
            if done:
                # æ‰“å°ä¿¡æ¯
                print(f"Process: {rank} Update: {counter.value} | Ep_r: {episode_reward:.0f}")
                print('------------------------------------------------------')
                
                # æ›´æ–°å¹³å‡åˆ†
                flag_finish, scores_avg_new = print_avg(scores, rank, episode_reward, lock, avg_ep, params, False, scores_avg)
                scores_avg = scores_avg_new
                
                if flag_finish:
                    # ä¿å­˜æ¨¡å‹
                    print('Save Model...')
                    if params['env_name'] == 'PongNoFrameskip-v4':
                        torch.save(shared_model, './saved_model/shared_model_pong.pt')
                    elif params['env_name'] == 'BreakoutNoFrameskip-v4':
                        torch.save(shared_model, './saved_model/shared_model_break.pt')
                    plot_avg_scores(scores_avg, 'Plot AVG Scores')
                    
                    with flag_exit.get_lock():
                        flag_exit.value = 1
                    return
                
                # é‡ç½®
                episode_length = 0
                episode_reward = 0
                reset_out = env.reset()
                obs = reset_out[0] if isinstance(reset_out, tuple) else reset_out
                if obs.dtype != np.uint8:
                    obs = (obs * 255).astype(np.uint8) if obs.max() <= 1.0 else obs.astype(np.uint8)
                state = torch.from_numpy(obs)
            else:
                # æ›´æ–°çŠ¶æ€
                obs = next_obs
                if obs.dtype != np.uint8:
                    obs = (obs * 255).astype(np.uint8) if obs.max() <= 1.0 else obs.astype(np.uint8)
                state = torch.from_numpy(obs)
            
            # ä¿å­˜value, log_prob, reward
            values.append(value)
            log_probs.append(log_prob_action)
            rewards.append(reward_clipped)
            
            if done:
                break
        
        # è®¡ç®—bootstrap value R
        R = torch.zeros(1, 1)
        if not done:
            state_tensor = state.unsqueeze(0).permute(0, 3, 1, 2)
            with torch.no_grad():
                _, value, _ = model((state_tensor, (hx, cx)))
            R = value.detach()
        
        values.append(R)
        
        # è®¡ç®—æŸå¤±
        policy_loss = 0
        value_loss = 0
        gae = torch.zeros(1, 1)
        
        # ä»åå¾€å‰è®¡ç®— (è¿™æ˜¯A3Cçš„æ ¸å¿ƒ!)
        for i in reversed(range(len(rewards))):
            R = params['gamma'] * R + rewards[i]
            advantage = R - values[i]
            value_loss = value_loss + 0.5 * advantage.pow(2)
            
            # GAE (Generalized Advantage Estimation)
            delta_t = rewards[i] + params['gamma'] * values[i + 1] - values[i]
            gae = gae * params['gamma'] * params.get('gae_lambda', 0.95) + delta_t
            
            # Policy loss = -log_prob * advantage - entropy_coef * entropy
            # æ³¨æ„: ç†µæ˜¯æ­£çš„ï¼Œæˆ‘ä»¬è¦æœ€å¤§åŒ–ç†µï¼Œæ‰€ä»¥ç”¨å‡å·
            policy_loss = policy_loss - log_probs[i] * gae.detach() - params['entropy_coef'] * entropies[i]
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss = policy_loss + params['value_coeff'] * value_loss
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), params['max_grad_norm'])
        
        # å¤åˆ¶æ¢¯åº¦åˆ°å…±äº«æ¨¡å‹
        ensure_shared_grads(model, shared_model)
        
        # æ›´æ–°å…±äº«æ¨¡å‹
        optimizer.step()
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if counter.value % 500 == 0:
            print(f'Worker: {rank} | Update: {counter.value}')
            print(f'  Policy Loss: {policy_loss.item():.4f}')
            print(f'  Value Loss: {value_loss.item():.4f}')
            print(f'  Total Loss: {total_loss.item():.4f}')
            print('------------------------------------------------------')


def print_avg(scores, p_i, tot_rew, lock, avg_ep, params, flag_finish, array_avgs):
    """è®¡ç®—å¹¶æ‰“å°å¹³å‡åˆ†æ•°"""
    with lock:
        scores.append([p_i, tot_rew])
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†ä¸€ä¸ªepisode
        all_found = 0
        for p_k in range(params['n_process']):
            for s_k in scores:
                if p_k == s_k[0]:
                    all_found += 1
                    break
        
        if all_found == params['n_process']:
            avg = 0
            for p_j in range(params['n_process']):
                for idx, s_i in enumerate(list(scores)):
                    if p_j == s_i[0]:
                        avg += s_i[1]
                        scores.remove(s_i)
                        break
            
            with avg_ep.get_lock():
                avg_ep.value += 1
                avg_score = avg / params['n_process']
                print('\n------------ AVG -------------')
                print(f"Ep: {avg_ep.value} | AVG: {avg_score:.2f}")
                print('------------------------------\n')
                array_avgs.append(avg_score)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                max_episodes = params.get('max_episodes', 1000)
                
                if len(array_avgs) >= 100:
                    recent_avg = np.mean(np.array(array_avgs[-100:]))
                    print(f'AVG last 100 scores: {recent_avg:.2f}')
                    print(f'Progress: {avg_ep.value}/{max_episodes} episodes\n')
                    
                    if recent_avg >= params['mean_reward']:
                        flag_finish = True
                        print('========================')
                        print('ğŸ‰ TARGET REACHED!')
                        print('========================')
                elif avg_ep.value >= max_episodes:
                    flag_finish = True
                    print('========================')
                    print('âš ï¸ MAX EPISODES REACHED')
                    print('========================')
    
    return flag_finish, array_avgs
