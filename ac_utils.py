import torch
import torch.nn.functional as F
from utils import *

def compute_log_prob_actions(logits):
    prob_v = F.softmax(logits, dim=-1)
    log_prob_v = F.log_softmax(logits, dim=-1)
    dist = torch.distributions.Categorical(probs=prob_v)
    action = dist.sample()
    action_log_prob = log_prob_v[0, action.item()]
    return action.item(), prob_v, log_prob_v, action_log_prob


def rollout(p_i, counter, params, model, hx, cx, frame_queue, env, current_state, episode_length, actions_name, layers_, tot_rew, scores, lock, avg_ep, scores_avg):
    
    #empty lists
    values = []
    log_probs = []
    rewards = []
    entropies = []
    dones = []  # Ê∑ªÂä†doneÊ†áÂøóÂàóË°®
    
    flag_finish = False
    done = False  # ÂàùÂßãÂåñdone
    
    for _ in range(params['rollout_size']):
        episode_length +=1
        
        current_state = current_state.unsqueeze(0).permute(0,3,1,2)
        
        # ËÆ°ÁÆólogits, valueÂíåLSTMÁä∂ÊÄÅ
        logits, value, (hx, cx) = model((current_state, (hx, cx)))
        
        # ËÆ°ÁÆóÊ¶ÇÁéáÂíåÈááÊ†∑Âä®‰Ωú
        prob = F.softmax(logits, dim=-1)
        log_prob = F.log_softmax(logits, dim=-1)
        entropy = -(log_prob * prob).sum(1, keepdim=True)
        
        action = prob.multinomial(num_samples=1).detach()
        action_log_prob = log_prob.gather(1, action)
        
        # ÊâßË°åÂä®‰Ωú
        next_frame, reward, done, _ = skip_frames(action.item(), env, skip_frame=4)
        
        # Â≠òÂÇ®
        values.append(value)
        log_probs.append(action_log_prob)
        rewards.append(np.sign(reward))  # Â•ñÂä±Ë£ÅÂâ™Âà∞[-1, 1]
        entropies.append(entropy)
        dones.append(done)  # ‰øùÂ≠òdoneÊ†áÂøó
        
        tot_rew += reward
        frame_queue.append(frame_preprocessing(next_frame))
        next_state = stack_frames(frame_queue)
        current_state = next_state
        
        if episode_length > params['max_ep_length']:
            break
        
        if done:
            #reset env
            in_state_i = env.reset()
            frame_queue = initialize_queue(frame_queue, layers_['n_frames'], in_state_i, env, actions_name)
            input_frames = stack_frames(frame_queue)
            current_state = input_frames
            episode_length = 0
            print(
                "Process: ", p_i,
                "Update:", counter.value,
                "| Ep_r: %.0f" % tot_rew,
            )
            print('------------------------------------------------------')
            flag_finish, scores_avg = print_avg(scores, p_i, tot_rew, lock, avg_ep, params, flag_finish, scores_avg)                        
            print('\n')
            if flag_finish == True:
                break
            
            tot_rew = 0
            hx = torch.zeros(1, layers_['lstm_dim'])
            cx = torch.zeros(1, layers_['lstm_dim'])
        
    # bootstrapping - Â¶ÇÊûúÊúÄÂêé‰∏ÄÊ≠•episodeÁªìÊùü‰∫ÜÔºåR=0ÔºõÂê¶ÂàôÁî®ÁΩëÁªú‰º∞ËÆ°
    if done:
        R = torch.zeros(1, 1)
    else:
        with torch.no_grad():
            _, R, _ = model((current_state.unsqueeze(0).permute(0,3,1,2), (hx, cx)))
    
    steps_array = (values, log_probs, rewards, entropies, dones, R)
    
    return hx, cx, steps_array, episode_length, frame_queue, current_state, tot_rew, counter, flag_finish, scores_avg


def compute_returns(steps_array, gamma, model):
    """ËÆ°ÁÆóreturnsÂíålosses - Ê≠£Á°ÆÂ§ÑÁêÜepisodeËæπÁïå"""
    values, log_probs, rewards, entropies, dones, R = steps_array
    
    R = R.detach()  # bootstrap value
    
    # ‰ªéÂêéÂæÄÂâçËÆ°ÁÆóreturnsÔºåÊ≠£Á°ÆÂ§ÑÁêÜepisodeËæπÁïå
    returns = []
    for i in reversed(range(len(rewards))):
        # Â¶ÇÊûúËøô‰∏ÄÊ≠•episodeÁªìÊùü‰∫ÜÔºåRÈáçÁΩÆ‰∏∫0ÂÜçËÆ°ÁÆó
        if dones[i]:
            R = torch.zeros(1, 1)
        R = rewards[i] + gamma * R
        returns.insert(0, R)
    
    # ËÆ°ÁÆóadvantages
    advantages = []
    for i in range(len(returns)):
        adv = returns[i] - values[i].detach()
        advantages.append(adv)
    
    # Ê†áÂáÜÂåñadvantagesÔºàÈùûÂ∏∏ÈáçË¶ÅÔºÅÔºâ
    if len(advantages) > 1:
        advantages_tensor = torch.cat(advantages)
        adv_mean = advantages_tensor.mean()
        adv_std = advantages_tensor.std() + 1e-8
    else:
        adv_mean = 0
        adv_std = 1
    
    # ËÆ°ÁÆólosses
    policy_loss = 0
    value_loss = 0
    entropy_sum = 0
    
    for i in range(len(rewards)):
        # Ê†áÂáÜÂåñÁöÑadvantage
        normalized_adv = (advantages[i] - adv_mean) / adv_std
        
        # Policy loss
        policy_loss = policy_loss - log_probs[i] * normalized_adv.detach()
        
        # Value loss
        value_loss = value_loss + 0.5 * (returns[i].detach() - values[i]).pow(2)
        
        # Entropy
        entropy_sum = entropy_sum + entropies[i]
    
    # entropy_loss: Ë¥üÁÜµÔºåÂä†Âà∞loss‰∏≠Áõ∏ÂΩì‰∫éÈºìÂä±Êé¢Á¥¢
    entropy_loss = -entropy_sum
    
    return policy_loss, value_loss, entropy_loss
    
    
def ensure_shared_grads(local_model, shared_model):
    """Â∞ÜÊú¨Âú∞Ê®°ÂûãÁöÑÊ¢ØÂ∫¶Â§çÂà∂Âà∞ÂÖ±‰∫´Ê®°Âûã"""
    for param, shared_param in zip(local_model.parameters(), shared_model.parameters()):
        if param.grad is not None:
            if shared_param.grad is None:
                shared_param.grad = param.grad.clone()
            else:
                shared_param.grad += param.grad 
    

def update_parameters(probs, log_probs, action_log_probs, advantages, returns, values, value_coeff, entropy_coef):
    # Ê†áÂáÜÂåñ‰ºòÂäøÂáΩÊï∞ - ÂáèÂ∞ëÊñπÂ∑ÆÔºåÁ®≥ÂÆöËÆ≠ÁªÉ
    adv_normalized = advantages.detach()  # ÂØπ‰∫épolicy lossÔºåadvantage‰∏çÈúÄË¶ÅÊ¢ØÂ∫¶
    if adv_normalized.numel() > 1:
        adv_normalized = (adv_normalized - adv_normalized.mean()) / (adv_normalized.std() + 1e-8)
    
    #policy loss (‰ΩøÁî®Ê†áÂáÜÂåñÁöÑadvantageÔºåÂπ∂detachÈò≤Ê≠¢Ê¢ØÂ∫¶ÊµÅÂêëvalueÁΩëÁªú)
    policy_loss = -(action_log_probs * adv_normalized).mean() 
    
    #value loss (returnsÊòØÁõÆÊ†áÔºåÊ≤°ÊúâÊ¢ØÂ∫¶ÔºõvaluesÊúâÊ¢ØÂ∫¶)
    value_loss = torch.nn.functional.mse_loss(values, returns)
    
    #entropy loss (Ë¥üÁöÑÁÜµÔºåÂõ†‰∏∫Êàë‰ª¨ÊÉ≥ÊúÄÂ§ßÂåñÁÜµÊù•‰øùÊåÅÊé¢Á¥¢)
    entropy_loss = (probs * log_probs).sum(dim=1).mean()
    
    a3c_loss = policy_loss + value_coeff * value_loss + entropy_coef * entropy_loss
    
    return a3c_loss, value_loss, policy_loss, entropy_loss
    
def print_avg(scores, p_i, tot_rew, lock, avg_ep, params, flag_finish, array_avgs):
    print('\n')
    with lock:
        scores.append([p_i, tot_rew])
        #print('scores', scores)
        all_found = 0
        #check if all process present
        for p_k in range(0, params['n_process']):
            ff = False
            for s_k in scores:
                if p_k == s_k[0] and ff==False:
                    all_found+=1
                    ff = True
                
        if all_found == params['n_process']:
            avg = 0
            for p_j in range(0, params['n_process']):
                idx = 0
                found = False
                for s_i in scores:
                    if p_j == s_i[0] and found==False:
                        avg += s_i[1]
                        found=True
                        scores.pop(idx)
                    idx+=1
                    
            with avg_ep.get_lock():
                avg_ep.value +=1
                print('\n')
                print('------------ AVG-------------')
                print(f"Ep: {avg_ep.value} | AVG: {avg/params['n_process']}")
                print('-----------------------------')
                array_avgs.append(avg/params['n_process'])
                
                # ÂÆâÂÖ®Êú∫Âà∂ÔºöËÆæÁΩÆÊúÄÂ§ßËÆ≠ÁªÉepisodeÊï∞ÔºàÈªòËÆ§1000Ôºâ
                max_episodes = params.get('max_episodes', 1000)
                
                if len(array_avgs)>100:
                    avg = np.mean(np.array(array_avgs[-100:]))
                    print('\n')
                    print('------------------------------')
                    print(f'AVG last 100 scores: {avg:.2f}')
                    print(f'Progress: {avg_ep.value}/{max_episodes} episodes')
                    print('------------------------------')
                    print('\n')
                    if avg >= params['mean_reward']:
                        flag_finish = True
                        print('========================')
                        print('üéâ TARGET REACHED!')
                        print('========================')
                    elif avg_ep.value >= max_episodes:
                        flag_finish = True
                        print('========================')
                        print('‚ö†Ô∏è MAX EPISODES REACHED')
                        print(f'Final avg score: {avg:.2f}')
                        print('========================')
                else:
                    # Âç≥‰ΩøÊ≤°Âà∞100‰∏™episodeÔºå‰πüÊ£ÄÊü•ÊòØÂê¶Ë∂ÖËøáÊúÄÂ§ßÈôêÂà∂
                    if avg_ep.value >= max_episodes:
                        flag_finish = True
                        print('========================')
                        print('‚ö†Ô∏è MAX EPISODES REACHED')
                        print('========================')
                    else:
                        flag_finish = False
        else:
            print('Not enough process completed to compute AVG...')
            flag_finish = False
        
        return flag_finish, array_avgs