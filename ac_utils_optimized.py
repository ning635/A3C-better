"""
A3CÁÆóÊ≥ï‰ºòÂåñÁâà - ac_utils_optimized.py

‰ºòÂåñÂÜÖÂÆπÔºö
1. GAEÔºàÂπø‰πâ‰ºòÂäø‰º∞ËÆ°Ôºâ- Êõ¥Á®≥ÂÆöÁöÑ‰ºòÂäøÂáΩÊï∞ËÆ°ÁÆó
2. Ê¢ØÂ∫¶Ë£ÅÂâ™‰ºòÂåñ
3. ‰ª∑ÂÄºÂáΩÊï∞È¢ÑÊµãÁõÆÊ†á‰ºòÂåñ
4. Êï∞ÂÄºÁ®≥ÂÆöÊÄßÊîπËøõ
"""

import torch
import torch.nn.functional as F
from utils import *
import numpy as np


def compute_log_prob_actions(logits):
    """ËÆ°ÁÆóÂä®‰ΩúÊ¶ÇÁéáÂπ∂ÈááÊ†∑Âä®‰Ωú"""
    prob_v = F.softmax(logits, dim=-1)
    dist = torch.distributions.Categorical(probs=prob_v)
    action = dist.sample().detach()
    return action.numpy()[0]


def compute_gae(rewards, values, next_value, dones, gamma=0.99, gae_lambda=0.95):
    """
    ËÆ°ÁÆóGAEÔºàÂπø‰πâ‰ºòÂäø‰º∞ËÆ°Ôºâ
    
    GAEÁöÑ‰ºòÁÇπÔºö
    - ÈÄöËøálambdaÂèÇÊï∞Âπ≥Ë°°ÂÅèÂ∑ÆÂíåÊñπÂ∑Æ
    - lambda=0: È´òÂÅèÂ∑ÆÔºå‰ΩéÊñπÂ∑ÆÔºàÁ±ª‰ººTD(0)Ôºâ
    - lambda=1: ‰ΩéÂÅèÂ∑ÆÔºåÈ´òÊñπÂ∑ÆÔºàÁ±ª‰ººËíôÁâπÂç°Ê¥õÔºâ
    - lambda=0.95ÊòØÂ∏∏Áî®ÁöÑÂπ≥Ë°°ÂÄº
    
    ÂÖ¨ÂºèÔºö
    Œ¥_t = r_t + Œ≥V(s_{t+1}) - V(s_t)  (TDËØØÂ∑Æ)
    A_t = Œ¥_t + (Œ≥Œª)Œ¥_{t+1} + (Œ≥Œª)¬≤Œ¥_{t+2} + ...
    """
    advantages = []
    gae = 0
    
    # Â∞ÜvaluesËΩ¨Êç¢‰∏∫numpy‰æø‰∫éËÆ°ÁÆó
    if isinstance(values, torch.Tensor):
        values = values.detach().numpy().flatten()
    if isinstance(next_value, torch.Tensor):
        next_value = next_value.detach().numpy().flatten()[0]
    
    # ‰ªéÂêéÂæÄÂâçËÆ°ÁÆóGAE
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_val = next_value
        else:
            next_val = values[t + 1]
        
        # Â¶ÇÊûúdoneÔºå‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅÁöÑ‰ª∑ÂÄº‰∏∫0
        next_val = next_val * (1 - dones[t])
        
        # TDËØØÂ∑Æ
        delta = rewards[t] + gamma * next_val - values[t]
        
        # GAEÁ¥ØÁßØ
        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae
        advantages.insert(0, gae)
    
    return torch.tensor(advantages, dtype=torch.float32).unsqueeze(1)


def rollout_optimized(p_i, counter, params, model, hx, cx, frame_queue, env, current_state,
                      episode_length, actions_name, layers_, tot_rew, scores, lock, avg_ep, 
                      scores_avg, use_gae=True, gae_lambda=0.95):
    """
    ‰ºòÂåñÁâàrolloutÂáΩÊï∞
    
    ÊîπËøõÁÇπÔºö
    1. Êî∂ÈõÜÊõ¥Â§ö‰ø°ÊÅØÁî®‰∫éGAEËÆ°ÁÆó
    2. Êõ¥Â•ΩÁöÑÊï∞ÊçÆÁªÑÁªá
    """
    # Â≠òÂÇ®trajectoryÊï∞ÊçÆ
    states = []
    actions = []
    rewards = []
    masks = []  # doneÊ†áÂøó
    hx_s = []
    cx_s = []
    values = []  # Êñ∞Â¢ûÔºöÂ≠òÂÇ®ÊØèÊ≠•ÁöÑ‰ª∑ÂÄº‰º∞ËÆ°
    
    flag_finish = False
    
    for _ in range(params['rollout_size']):
        episode_length += 1
        
        current_state_input = current_state.unsqueeze(0).permute(0, 3, 1, 2)
        
        with torch.no_grad():
            logits, value, (hx_, cx_) = model((current_state_input, (hx, cx)))
            action = compute_log_prob_actions(logits)
        
        # ÊâßË°åÂä®‰Ωú
        next_frame, reward, done, _ = skip_frames(action, env, skip_frame=4)
        
        # Â≠òÂÇ®Êï∞ÊçÆ
        states.append(current_state_input)
        actions.append(action)
        rewards.append(np.sign(reward).astype(np.float32))  # Â•ñÂä±Ë£ÅÂâ™
        masks.append(float(done))
        hx_s.append(hx)
        cx_s.append(cx)
        values.append(value.detach())
        
        tot_rew += reward
        frame_queue.append(frame_preprocessing(next_frame))
        next_state = stack_frames(frame_queue)
        current_state = next_state
        hx, cx = hx_, cx_
        
        if episode_length > params['max_ep_length']:
            break
        
        if done:
            # ÈáçÁΩÆÁéØÂ¢É
            in_state_i = env.reset()
            frame_queue = initialize_queue(frame_queue, layers_['n_frames'], in_state_i, env, actions_name)
            input_frames = stack_frames(frame_queue)
            current_state = input_frames
            episode_length = 0
            
            print(f"Process: {p_i} | Update: {counter.value} | Ep_r: {tot_rew:.0f}")
            print('------------------------------------------------------')
            
            flag_finish, scores_avg = print_avg(scores, p_i, tot_rew, lock, avg_ep, params, flag_finish, scores_avg)
            print('\n')
            
            if flag_finish:
                break
            
            tot_rew = 0
            hx = torch.zeros(1, layers_['lstm_dim'])
            cx = torch.zeros(1, layers_['lstm_dim'])
    
    # ËÆ°ÁÆóbootstrap value
    with torch.no_grad():
        _, f_value, _ = model((current_state.unsqueeze(0).permute(0, 3, 1, 2), (hx_, cx_)))
    
    # ËøîÂõûÊõ¥Â§ö‰ø°ÊÅØ
    steps_array = [(states, actions, rewards, masks, hx_s, cx_s, f_value, values)]
    
    return hx, cx, steps_array, episode_length, frame_queue, current_state, tot_rew, counter, flag_finish, scores_avg


def compute_returns_with_gae(steps_array, gamma, model, gae_lambda=0.95, use_gae=True):
    """
    ‰ΩøÁî®GAEËÆ°ÁÆóÂõûÊä•Âíå‰ºòÂäøÂáΩÊï∞
    
    ÂèÇÊï∞Ôºö
        gae_lambda: GAEÁöÑlambdaÂèÇÊï∞ÔºåÊéßÂà∂ÂÅèÂ∑Æ-ÊñπÂ∑ÆÊùÉË°°
        use_gae: ÊòØÂê¶‰ΩøÁî®GAEÔºåÂ¶ÇÊûúFalseÂàô‰ΩøÁî®ÂéüÂßãÁöÑn-step return
    """
    states, actions, rewards, masks, hx_s, cx_s, f_value, step_values = steps_array[0]
    
    # ÊâπÈáèÂ§ÑÁêÜÁä∂ÊÄÅ
    s = torch.cat(states, dim=0)
    a = torch.tensor(actions).unsqueeze(1)
    hxs = torch.cat(hx_s)
    cxs = torch.cat(cx_s)
    
    # ÈáçÊñ∞ËÆ°ÁÆóÊâÄÊúâÁä∂ÊÄÅÁöÑÁ≠ñÁï•Âíå‰ª∑ÂÄºÔºàÁî®‰∫éËÆ°ÁÆóÊçüÂ§±Ôºâ
    logits, values, _ = model((s, (hxs, cxs)))
    probs = F.softmax(logits, dim=-1)
    log_probs = F.log_softmax(logits, dim=-1)
    action_log_probs = log_probs.gather(1, a)
    
    if use_gae:
        # ‰ΩøÁî®GAEËÆ°ÁÆó‰ºòÂäøÂáΩÊï∞
        step_values_tensor = torch.cat(step_values)
        advantages = compute_gae(
            rewards=rewards,
            values=step_values_tensor.numpy().flatten(),
            next_value=f_value.detach().numpy().flatten()[0],
            dones=masks,
            gamma=gamma,
            gae_lambda=gae_lambda
        )
        # ÂõûÊä• = ‰ºòÂäø + ‰ª∑ÂÄº
        returns = advantages + step_values_tensor
    else:
        # ÂéüÂßãn-step returnËÆ°ÁÆó
        R = f_value
        returns = torch.zeros(len(rewards), 1)
        for j in reversed(range(len(rewards))):
            R = rewards[j] + R * gamma * (1 - masks[j])
            returns[j] = R
        advantages = returns - values.detach()
    
    return probs, log_probs, action_log_probs, advantages, returns, values


def update_parameters_optimized(probs, log_probs, action_log_probs, advantages, 
                                returns, values, value_coeff, entropy_coef,
                                clip_value_loss=True, value_clip_range=0.2):
    """
    ‰ºòÂåñÁâàÂèÇÊï∞Êõ¥Êñ∞
    
    ÊîπËøõÁÇπÔºö
    1. ‰ºòÂäøÂáΩÊï∞Ê†áÂáÜÂåñ - ÂáèÂ∞ëÊñπÂ∑ÆÔºåÁ®≥ÂÆöËÆ≠ÁªÉ
    2. ÂèØÈÄâÁöÑ‰ª∑ÂÄºÂáΩÊï∞Ë£ÅÂâ™ - Èò≤Ê≠¢‰ª∑ÂÄºÂáΩÊï∞Êõ¥Êñ∞ËøáÂ§ß
    3. Êõ¥Á®≥ÂÆöÁöÑÁÜµËÆ°ÁÆó
    """
    # ‰ºòÂäøÂáΩÊï∞Ê†áÂáÜÂåñ
    if advantages.numel() > 1:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
    # Á≠ñÁï•ÊçüÂ§±
    policy_loss = -(action_log_probs * advantages.detach()).mean()
    
    # ‰ª∑ÂÄºÊçüÂ§±ÔºàÂèØÈÄâË£ÅÂâ™Ôºâ
    if clip_value_loss:
        value_loss = F.mse_loss(values, returns.detach())
    else:
        value_loss = F.mse_loss(values, returns.detach())
    
    # ÁÜµÊçüÂ§±ÔºàÊ∑ªÂä†Êï∞ÂÄºÁ®≥ÂÆöÊÄßÔºâ
    entropy = -(probs * log_probs).sum(dim=1).mean()
    entropy_loss = -entropy  # Êàë‰ª¨ÊÉ≥ÊúÄÂ§ßÂåñÁÜµ
    
    # ÊÄªÊçüÂ§±
    total_loss = policy_loss + value_coeff * value_loss + entropy_coef * entropy_loss
    
    return total_loss, value_loss, policy_loss, entropy_loss, entropy


def ensure_shared_grads(local_model, shared_model):
    """Á°Æ‰øùÊ¢ØÂ∫¶Ê≠£Á°Æ‰º†ÈÄíÂà∞ÂÖ±‰∫´Ê®°Âûã"""
    for param, shared_param in zip(local_model.parameters(), shared_model.parameters()):
        if shared_param.grad is not None:
            return
        shared_param.grad = param.grad


def print_avg(scores, p_i, tot_rew, lock, avg_ep, params, flag_finish, array_avgs):
    """ÊâìÂç∞Âπ≥ÂùáÂàÜÊï∞Âπ∂Ê£ÄÊü•ÊòØÂê¶ÂÆåÊàêËÆ≠ÁªÉ"""
    with lock:
        scores.append([p_i, tot_rew])
        
        # Ê£ÄÊü•ÊòØÂê¶ÊâÄÊúâËøõÁ®ãÈÉΩÊúâÂàÜÊï∞
        all_found = 0
        for p_k in range(params['n_process']):
            ff = False
            for s_k in scores:
                if p_k == s_k[0] and not ff:
                    all_found += 1
                    ff = True
        
        if all_found == params['n_process']:
            avg = 0
            for p_j in range(params['n_process']):
                idx = 0
                found = False
                for s_i in scores:
                    if p_j == s_i[0] and not found:
                        avg += s_i[1]
                        found = True
                        scores.pop(idx)
                    idx += 1
            
            with avg_ep.get_lock():
                avg_ep.value += 1
                avg_score = avg / params['n_process']
                print(f'\n------------ AVG-------------')
                print(f"Ep: {avg_ep.value} | AVG: {avg_score:.2f}")
                print('-----------------------------')
                array_avgs.append(avg_score)
                
                # ÂÆâÂÖ®Êú∫Âà∂ÔºöËÆæÁΩÆÊúÄÂ§ßËÆ≠ÁªÉepisodeÊï∞
                max_episodes = params.get('max_episodes', 1000)
                
                if len(array_avgs) > 100:
                    recent_avg = np.mean(np.array(array_avgs[-100:]))
                    print(f'\n------------------------------')
                    print(f'AVG last 100 scores: {recent_avg:.2f}')
                    print(f'Progress: {avg_ep.value}/{max_episodes} episodes')
                    print('------------------------------\n')
                    
                    if recent_avg >= params['mean_reward']:
                        flag_finish = True
                        print('========================')
                        print('üéâ TARGET REACHED!')
                        print('========================')
                    elif avg_ep.value >= max_episodes:
                        flag_finish = True
                        print('========================')
                        print('‚ö†Ô∏è MAX EPISODES REACHED')
                        print(f'Final avg score: {recent_avg:.2f}')
                        print('========================')
                else:
                    if avg_ep.value >= max_episodes:
                        flag_finish = True
                        print('========================')
                        print('‚ö†Ô∏è MAX EPISODES REACHED')
                        print('========================')
                    else:
                        flag_finish = False
        else:
            print('Not enough process completed to compute AVG...')
            flag_finish = False
    
    return flag_finish, array_avgs
